# BeachWatch - Multi-Beach Webcam Scraper

A production-ready Playwright-based scraper for capturing screenshots from multiple Sydney beach webcams. Part of the BeachWatch MVP for real-time beach crowd monitoring.

## Overview

This scraper captures timestamped screenshots from Sydney beach webcams for crowd analysis and monitoring. It supports multiple beaches with configurable webcam sources and built-in fallback mechanisms.

### Supported Beaches (MVP)

- ‚úÖ **Bondi Beach** - 2 webcam sources
- ‚úÖ **Manly Beach** - 1 webcam source
- ‚úÖ **Coogee Beach** - 1 webcam source
- ‚úÖ **Maroubra Beach** - 2 webcam sources
- ‚è≠Ô∏è **Bronte Beach** - Coming soon (limited webcam availability)

## Features

- üèñÔ∏è Multi-beach support with priority-based fallback
- üì∏ Timestamped screenshot capture
- ‚öôÔ∏è JSON-based configuration
- üîÑ Automatic retry mechanism
- üìä Detailed success/failure reporting
- üéØ CLI with beach filtering
- üöÄ npm scripts for common operations
- üì¶ Modular architecture for easy extension

## Installation

1. Clone the repository:
```bash
git clone https://github.com/jayphen/beach-crowd.git
cd beach-crowd
```

2. Install dependencies:
```bash
npm install
```

3. Install Playwright browsers:
```bash
npm run install:browsers
```

## Quick Start

### Scrape all enabled beaches:
```bash
npm run scrape
```

### Scrape specific beaches:
```bash
# Using npm scripts
npm run scrape:bondi
npm run scrape:manly
npm run scrape:coogee
npm run scrape:maroubra

# Using CLI directly
node multi-beach-scraper.js bondi,manly
node multi-beach-scraper.js --beaches bondi,coogee
```

### View help:
```bash
node multi-beach-scraper.js --help
```

## Configuration

All beach and webcam configurations are stored in `beaches-config.json`:

```json
{
  "beaches": [
    {
      "id": "bondi",
      "name": "Bondi Beach",
      "location": "Waverley Council, Eastern Suburbs",
      "enabled": true,
      "webcams": [
        {
          "id": "bondi-surf-club",
          "name": "Bondi Surf Club",
          "url": "https://bondisurfclub.com/bondi-surf-cam/",
          "priority": 1,
          "notes": "Primary webcam source"
        }
      ]
    }
  ],
  "scraper_config": {
    "viewport": { "width": 1920, "height": 1080 },
    "timeout": 30000,
    "wait_for_load": 5000,
    "screenshots_dir": "./screenshots",
    "headless": true,
    "retry_attempts": 2,
    "retry_delay": 3000
  }
}
```

### Adding a New Beach

1. Edit `beaches-config.json`
2. Add a new beach object to the `beaches` array:
```json
{
  "id": "your-beach-id",
  "name": "Your Beach Name",
  "location": "Location Details",
  "enabled": true,
  "webcams": [
    {
      "id": "webcam-id",
      "name": "Webcam Name",
      "url": "https://example.com/webcam",
      "priority": 1,
      "notes": "Optional notes"
    }
  ]
}
```

3. Run the scraper:
```bash
node multi-beach-scraper.js your-beach-id
```

## Output

Screenshots are saved to the `screenshots/` directory with the following naming convention:

```
{beachId}_{webcamId}_YYYY-MM-DD_HH-MM-SS.png
```

Examples:
- `bondi_bondi-surf-club_2026-01-22_14-23-41.png`
- `manly_manly-pacific_2026-01-22_14-25-15.png`

## How It Works

1. **Load Configuration**: Reads beach and webcam definitions from `beaches-config.json`
2. **Launch Browser**: Starts Playwright Chromium in headless mode
3. **Navigate**: Visits each enabled webcam URL
4. **Wait**: Allows time for webcam streams to load (5 seconds default)
5. **Capture**: Takes viewport screenshot (1920x1080)
6. **Save**: Stores image with timestamped filename
7. **Report**: Generates detailed success/failure summary

## CLI Usage

```bash
node multi-beach-scraper.js [OPTIONS]

OPTIONS:
  --beaches, -b <list>   Comma-separated list of beach IDs
  --help, -h             Show help message

EXAMPLES:
  # Scrape all enabled beaches
  node multi-beach-scraper.js

  # Scrape specific beaches
  node multi-beach-scraper.js bondi,manly
  node multi-beach-scraper.js --beaches coogee
```

## npm Scripts

| Command | Description |
|---------|-------------|
| `npm run scrape` | Scrape all enabled beaches |
| `npm run scrape:bondi` | Scrape Bondi Beach only |
| `npm run scrape:manly` | Scrape Manly Beach only |
| `npm run scrape:coogee` | Scrape Coogee Beach only |
| `npm run scrape:maroubra` | Scrape Maroubra Beach only |
| `npm run scrape:all` | Scrape all enabled beaches |
| `npm run install:browsers` | Install Playwright browsers |

## Module Usage

You can import and use the scraper functions in other Node.js scripts:

```javascript
const { scrapeAllBeaches, scrapeBeach } = require('./multi-beach-scraper');

async function main() {
  // Scrape all beaches
  const result = await scrapeAllBeaches();
  console.log(`Success: ${result.summary.successful_webcams}/${result.summary.total_webcams}`);

  // Scrape specific beaches
  const filtered = await scrapeAllBeaches(['bondi', 'manly']);

  // Access individual beach results
  result.beaches.forEach(beach => {
    console.log(`${beach.beach_name}: ${beach.successful_webcams}/${beach.total_webcams}`);
  });
}

main();
```

## Return Values

### scrapeAllBeaches()

```javascript
{
  success: true,
  total_duration: 45.67,
  beaches: [
    {
      beach_id: "bondi",
      beach_name: "Bondi Beach",
      location: "Waverley Council, Eastern Suburbs",
      webcams: [
        {
          success: true,
          filepath: "./screenshots/bondi_bondi-surf-club_2026-01-22_14-23-41.png",
          fileSize: 68673,
          duration: 17.82,
          timestamp: "2026-01-23T01:23:41.234Z"
        }
      ],
      total_webcams: 2,
      successful_webcams: 2,
      failed_webcams: 0
    }
  ],
  summary: {
    processed: 4,
    skipped: 1,
    total_webcams: 6,
    successful_webcams: 5,
    failed_webcams: 1
  },
  timestamp: "2026-01-23T01:30:00.000Z"
}
```

## Error Handling

The scraper handles common errors gracefully:
- Network timeouts
- Page load failures
- Missing webcam elements (falls back to full page capture)
- Browser launch issues
- Invalid configurations

Failed captures are logged but don't stop the scraper from processing other beaches.

## Performance

Typical performance metrics:
- **Single beach**: ~15-25 seconds
- **All beaches (4 enabled)**: ~45-60 seconds
- **Browser launch**: ~2 seconds
- **Page load + capture**: ~10-20 seconds per webcam

## Troubleshooting

### Issue: Browser fails to launch
```bash
npm run install:browsers
```

### Issue: Timeout errors
Increase `timeout` in `beaches-config.json`:
```json
"scraper_config": {
  "timeout": 60000
}
```

### Issue: Screenshots are blank/black
Increase `wait_for_load` to allow more time for streams:
```json
"scraper_config": {
  "wait_for_load": 10000
}
```

### Issue: Permission denied
Ensure the screenshots directory is writable:
```bash
chmod 755 screenshots/
```

## Project Structure

```
beach-crowd/
‚îú‚îÄ‚îÄ multi-beach-scraper.js    # Main scraper script
‚îú‚îÄ‚îÄ beaches-config.json        # Beach and webcam configuration
‚îú‚îÄ‚îÄ scraper.js                 # Legacy single-beach scraper (Bondi)
‚îú‚îÄ‚îÄ screenshots/               # Screenshot output directory
‚îú‚îÄ‚îÄ package.json               # npm configuration
‚îú‚îÄ‚îÄ README.md                  # This file
‚îú‚îÄ‚îÄ README-scraper.md          # Legacy scraper documentation
‚îî‚îÄ‚îÄ webcam-urls.md            # Research notes on webcam sources
```

## Next Steps

- [ ] Set up Cloudflare Workers for scheduled captures (every 10 minutes)
- [ ] Integrate YOLOv8 for person detection
- [ ] Store screenshots in Cloudflare R2
- [ ] Store metadata in Cloudflare D1 database
- [ ] Build API endpoints for serving data
- [ ] Create frontend with SvelteKit

## Testing Results

**Date**: 2026-01-23
**Beaches Tested**: Bondi Beach (2 webcams)
**Status**: ‚úÖ Working
**Success Rate**: 100%
**Average Duration**: ~23 seconds for 2 webcams
**Screenshot Quality**: High (67-1036 KB per image)

## Related Documentation

- `README-scraper.md` - Original Bondi Beach scraper documentation
- `webcam-urls.md` - Research notes on 15 Sydney beach webcams
- `beach-monitor-PRD.md` - BeachWatch product requirements
- `todolist.txt` - Project development roadmap

## License

ISC

## Contributing

This is part of the BeachWatch MVP project. For questions or contributions, please contact the BeachWatch team.

## Author

BeachWatch Development Team
